{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pidipidi/CS470_IAI_2022Fall/blob/main/assignment_3/CS470_Assignment3_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS470 Assignment 3\n",
        "In this assignment, we will first design a **Markov Decision Process (MDP)** for a *gridworld* environment. Then, on top of it, we implement and run **dynamic programming (DP)** approaches. Note that you must run this file on **Google Chrome**, otherwise you may not be able to play recorded videos."
      ],
      "metadata": {
        "id": "L96B5Ed06r8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements for initialization\n",
        "We will first install dependencies and declare auxiliary functions for visualization"
      ],
      "metadata": {
        "id": "tHYpkF-KbM1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMLNlciUp9y1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run the cell below to install dependencies\n",
        "\n",
        "  ##################################################################\n",
        "  ###################                             ##################\n",
        "  ###################   DO NOT MODIFY THIS CELL   ##################\n",
        "  ###################                             ##################\n",
        "  ##################################################################\n",
        "\n",
        "#Install dependencies to visualize agents \n",
        "!pip install pyglet==1.5.1  &> /dev/null\n",
        "!apt install -y python-opengl ffmpeg xvfb &> /dev/null\n",
        "!pip install pyvirtualdisplay &> /dev/null\n",
        "!pip install gym==0.23.0 &> /dev/null \n",
        "# !pip install pygame &> /dev/null\n",
        "!pip install numpy &> /dev/null\n",
        "\n",
        "# !pip install huggingface_hub &> /dev/null\n",
        "!pip install pickle5 &> /dev/null\n",
        "!pip install pyyaml==6.0 &> /dev/null \n",
        "!pip install imageio imageio_ffmpeg &> /dev/null\n",
        "\n",
        "!apt-get install -y python x11-utils &> /dev/null\n",
        "!pip install scikit-video ffio pyrender &> /dev/null\n",
        "# !pip install tensorflow_probability==0.12.0 &> /dev/null\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "import imageio, random, copy\n",
        "import sys, time, os, base64, io\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import IPython, functools, matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the cell below to declare auxiliary functions\n",
        "\n",
        "  ##################################################################\n",
        "  ###################                             ##################\n",
        "  ###################   DO NOT MODIFY THIS CELL   ##################\n",
        "  ###################                             ##################\n",
        "  ##################################################################\n",
        "\n",
        "from IPython.display import HTML\n",
        "def eval_policy(env, policy=None, num_episodes=10):\n",
        "    \"\"\"Evaluate a model (i.e., policy) running on the input environment\"\"\"\n",
        "    # env = gym.make(env_name)\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "    counter = 0\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    episode_reward = 0\n",
        "    episode_rewards = []\n",
        "    while num_runs < num_episodes:\n",
        "        if policy is not None:\n",
        "            action = policy(obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        prev_obs = obs\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        counter += 1\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            num_runs += 1\n",
        "            obs = env.reset()\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "    return episode_rewards\n",
        "\n",
        "def save_video_of_model(env_name, model=None, suffix=\"\", num_episodes=10):\n",
        "    \"\"\"\n",
        "    Record a video that shows the behavior of an agent following a model \n",
        "    (i.e., policy) on the input environment\n",
        "    \"\"\"\n",
        "    import skvideo.io\n",
        "    from pyvirtualdisplay import Display\n",
        "    display = Display(visible=0, size=(400, 300))\n",
        "    display.start()\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "\n",
        "    filename = env_name + suffix + \".mp4\"\n",
        "    output_video = skvideo.io.FFmpegWriter(filename)\n",
        "\n",
        "    counter = 0\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    returns = 0\n",
        "    while num_runs < num_episodes:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        output_video.writeFrame(frame)\n",
        "\n",
        "        if \"Gridworld\" in env_name:\n",
        "            input_obs = obs\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown env for saving: {env_name}\")\n",
        "\n",
        "        if model is not None:\n",
        "            action = model(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "\n",
        "        prev_obs = obs\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        counter += 1\n",
        "        returns += reward\n",
        "        if done:\n",
        "            num_runs += 1\n",
        "            obs = env.reset()\n",
        "\n",
        "    output_video.close()\n",
        "    print(\"Successfully saved {} frames into {}!\".format(counter, filename))\n",
        "    return filename, returns / num_runs\n",
        "\n",
        "def play_video(filename, width=None):\n",
        "    \"\"\"Play the input video\"\"\"\n",
        "\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(filename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    html = \"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url\n",
        "    return  html\n",
        "\n",
        "\n",
        "def render_value_map_with_action(env, Q, policy=None):\n",
        "    '''\n",
        "    Render a state (or action) value grid map.\n",
        "    V[s] = max(Q[s,a])\n",
        "    '''\n",
        "    Q = Q.copy()\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    if len(np.shape(Q))>1:\n",
        "        V = np.amax(Q, axis=1) \n",
        "        V = V.reshape((n,m))\n",
        "    else:\n",
        "        V = Q.reshape((n,m))\n",
        "    import itertools\n",
        "    symbol = ['.', '^','v', '<', '>']\n",
        "    x = range(0, env.grid_map_shape[0]+1)\n",
        "    y = range(0, env.grid_map_shape[1]+1)\n",
        "\n",
        "    min_val = V[0,0]\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle // env.grid_map_shape[1]\n",
        "        posy = obstacle % env.grid_map_shape[1]\n",
        "        V[posx, posy] = min_val\n",
        "        obstacles[posx, posy] = 1\n",
        "\n",
        "    plt.imshow(V, cmap='jet', interpolation='nearest')\n",
        "    for s in range(env.observation_space.n):\n",
        "        twod_state = env.serial_to_twod(s)\n",
        "        state_inds = s\n",
        "        best_action = policy(s)\n",
        "        plt.plot([twod_state[1]], [twod_state[0]], marker=symbol[best_action], linestyle='none', color='k')\n",
        "\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "            \n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    plt.show()\n",
        "\n",
        "def collect_traj(env, policy=None, num_episodes=10):\n",
        "    \"\"\"Collect trajectories (rollouts) following the input policy\"\"\"\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    episode_rewards = []\n",
        "    episode_reward = 0\n",
        "    traj = []\n",
        "    trajs = []\n",
        "\n",
        "    while num_runs < num_episodes:\n",
        "        input_obs = obs\n",
        "        if policy is not None:\n",
        "            action = policy(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        traj.append(obs)\n",
        "        prev_obs = obs\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            num_runs += 1\n",
        "            trajs.append(traj)\n",
        "            traj = []\n",
        "            obs = env.reset()\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "    return trajs#, episode_rewards\n",
        "\n",
        "def plot_trajs(env, trajectories):\n",
        "    \"\"\"Plot the input trajectories\"\"\"\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    V = np.zeros([n,m])\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle // env.grid_map_shape[1]\n",
        "        posy = obstacle % env.grid_map_shape[1]\n",
        "        obstacles[posx, posy] = 1\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    for trajectory in trajectories:\n",
        "        traj_2d = np.array([ env.serial_to_twod(s) for s in trajectory ])\n",
        "        y = traj_2d[:, 0]\n",
        "        x = traj_2d[:, 1]\n",
        "        plt.plot(x, y, alpha=0.1, color='r')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MfKdFRBd30bO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Markov Decision Process\n",
        "In this problem, you design an MDP for a stochastic version of *grid-world* environment. You will implement *transition_model*, *compute_reward*, *is_done* and *step* functions following the rules defined in the assignment PDF."
      ],
      "metadata": {
        "id": "Moxb_9vubXU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  ##################################################################\n",
        "  ###################                             ##################\n",
        "  ###################   DO NOT MODIFY THIS CELL   ##################\n",
        "  ###################                             ##################\n",
        "  ##################################################################\n",
        "\n",
        "class BaseGridEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "    def __init__(self, size=[8,10], start=None ,epsilon=0.05, obstacle=None):\n",
        "        \"\"\"\n",
        "        An initialization function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: a list of integers\n",
        "            the dimension of 2D grid environment\n",
        "        start: integer\n",
        "            start state (i.e., location)\n",
        "        epsilon: float\n",
        "            the probability of taking random actions\n",
        "        obstacle: \n",
        "\n",
        "        \"\"\"\n",
        "        self.grid_map_shape = [size[0], size[1]]  # The size of the map\n",
        "        self.epsilon = epsilon  # action-failure probability\n",
        "        self.obstacles = obstacle   # list of states stating position of the obstacles\n",
        "        \n",
        "        ''' set observation space and action space '''\n",
        "        self.observation_space = spaces.Discrete( size[0] * size[1])\n",
        "        self.action_space = spaces.Discrete( 5 )\n",
        "    \n",
        "        self.start_state = 0\n",
        "        self.terminal_state = size[0] * size[1] - 1\n",
        "\n",
        "    def serial_to_twod(self, ind):\n",
        "        \"\"\"Convert a serialized state number to a 2D map's state coordinate\"\"\"\n",
        "        return np.array( [ ind // self.grid_map_shape[1], ind % self.grid_map_shape[1]])\n",
        "\n",
        "    def twod_to_serial(self, twod):\n",
        "        \"\"\"Convert a 2D map's state coordinate to a serialized state number\"\"\"\n",
        "        return np.array( twod[0]* self.grid_map_shape[1] + twod[1])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Rest the environment by initializaing the start state \"\"\"\n",
        "        self.observation = self.start_state\n",
        "        return self.observation\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        \"\"\"Render the agent state\"\"\"\n",
        "        pixel_size = 20\n",
        "        img = np.zeros([ pixel_size * self.grid_map_shape[0], pixel_size * self.grid_map_shape[1],3])\n",
        "        for obstacle in self.obstacles:\n",
        "          pos_x, pos_y = self.serial_to_twod(obstacle)\n",
        "          img[pixel_size*pos_x: pixel_size*(1+pos_x), pixel_size*pos_y: pixel_size*(1+pos_y)] += [255,0,0]\n",
        "        agent_state = self.serial_to_twod(self.observation)   \n",
        "        agent_target_state = self.serial_to_twod(self.terminal_state)\n",
        "        img[pixel_size*agent_state[0]: pixel_size*(1+agent_state[0]), pixel_size*agent_state[1]: pixel_size*(1+agent_state[1])] += [0,0,255]\n",
        "        img[pixel_size*agent_target_state[0]: pixel_size*(1+agent_target_state[0]), pixel_size*agent_target_state[1]: pixel_size*(1+agent_target_state[1])] += [0,255,0]\n",
        "        if mode == 'human':\n",
        "          fig = plt.figure(0)\n",
        "          plt.clf()\n",
        "          plt.imshow(img, cmap='gray')\n",
        "          fig.canvas.draw()\n",
        "          plt.pause(0.01)\n",
        "        if mode == 'rgb_array':\n",
        "          return img\n",
        "        return \n",
        "\n",
        "    def _close_env(self):\n",
        "        \"\"\"Close the environment screen\"\"\"\n",
        "        plt.close(1)\n",
        "        return\n"
      ],
      "metadata": {
        "id": "lGUcYuZ06x15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ed3bkDS5Q3K"
      },
      "outputs": [],
      "source": [
        "class GridEnv(BaseGridEnv):\n",
        "    \"\"\"\n",
        "    A grid-world environment.\n",
        "    \"\"\"\n",
        "    def transition_model(self, state, action):\n",
        "        \"\"\"\n",
        "        A transition model that return a list of probabilities of transitions\n",
        "        to next states when the agent select 'action' at the 'state': T(s' | s,a)\n",
        "\n",
        "        In our envrionemnt, if the state is in obstacles or in a goal, \n",
        "        it will stay in its state at any action\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index        \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        probs: numpy array with a length of {size of state space}\n",
        "            probabilities of transition to the next_state ...                    \n",
        "        \"\"\"\n",
        "        if not isinstance(state, int):\n",
        "            state = state.item()\n",
        "\n",
        "        # the transition probabilities to the next states\n",
        "        probs = np.zeros(self.observation_space.n)\n",
        "\n",
        "        # Left top is [0,0], \n",
        "        action_pos_dict = {0: [0,0], 1:[-1, 0], 2:[1,0], 3:[0,-1], 4:[0,1]}\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - You have to fill out the \"probs\" variable following the   ###\n",
        "        ###   the assignment-problem 1's transition model rules         ###        \n",
        "        ### - Hint: you have to find the next state based on the        ###\n",
        "        ###         available action at the current state               ###\n",
        "        ###\n",
        "        ### Example\n",
        "        ### --------\n",
        "        ### After some initializations...\n",
        "        ### for action in availabe actions:\n",
        "        ###     find a next state\n",
        "        ###     fill out probs[next state] \n",
        "        ###        \n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def compute_reward(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        A reward function that returns the total reward after selecting 'action'\n",
        "        at the 'state'. In this environment, \n",
        "        (a) If it reaches a goal state, it terminates returning a reward of +10\n",
        "        (b) If it reaches an obstacle, it terminates returning a penalty of -5\n",
        "        (c) For any action, it add a step penalty of -0.1\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        reward: float\n",
        "            a total reward value\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - fill out the reward variable                              ###\n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "        return reward\n",
        "    \n",
        "    def is_done(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        Return True when the agent is in a terminal state or obstacles, \n",
        "        otherwise return False\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        done: Bool\n",
        "            the result of termination or collision\n",
        "        \"\"\"\n",
        "        done = None\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - fill out the \"done\" variable                              ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return done \n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        A step function that applies the input action to the environment.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        action: integer\n",
        "            action index         \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        observation: integer\n",
        "            the outcome of the given action (i.e., next state)... s' ~ T(s'|s,a)\n",
        "        reward: float\n",
        "            the reward that would get for ... r(s, a, s')\n",
        "        done: Bool\n",
        "            the result signal of termination or collision\n",
        "        info: Dictionary\n",
        "            Information dictionary containing miscellaneous information...\n",
        "            (Do not need to implement info)\n",
        "\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        action = int(action)\n",
        "        \n",
        "        probs = self.transition_model(self.observation, action)\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - sample the next state considring the transition model     ###\n",
        "        ### - then, compute \"reward\" and \"done\"                         ###\n",
        "        ###   next_state = ...                                          ###\n",
        "        ###\n",
        "        ### Example                                                     ###\n",
        "        ### -----------                                                 ###\n",
        "        ### next state = ?\n",
        "        ### self.observation = ?\n",
        "        ### reward = self.compute_reward(?, action, self.observation)   ###\n",
        "        ### done = self.is_done(?, action, self.observation)            ###\n",
        "        \n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return (self.observation, reward, done, {})\n",
        "\n",
        "  ##################################################################\n",
        "  ###################                             ##################\n",
        "  ###################  DO NOT MODIFY CODE BELOW   #################\n",
        "  ###################                             ##################\n",
        "  ##################################################################\n",
        "\n",
        "from gym import register\n",
        "if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.05, 'size':[8, 10], 'obstacle':[32, 43, 62, 63, 44, 35, 26, 58, 49, 41, 57, 61, 31]}\n",
        ")\n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(a) Transition model / dynamics\n",
        "\n",
        "Following the problem description on the assignment PDF, implement the transition_model. After implementation, please run the cell below."
      ],
      "metadata": {
        "id": "-Vr04-7W-7Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_policy(state):\n",
        "    \"\"\"A dummy random policy\"\"\"\n",
        "    if state > 40:\n",
        "        return np.random.choice([1, 3], 1).item()\n",
        "    else:\n",
        "        return np.random.choice([2, 4], 1).item()\n",
        "\n",
        "# Plot the distribution of the collected trajectories\n",
        "trajs = collect_traj(env, policy=dummy_policy, num_episodes=300)\n",
        "plot_trajs(env, trajs)\n",
        "\n",
        "# Plot the histogram of returns after running a dummy policy\n",
        "returns = eval_policy(env, policy=dummy_policy, num_episodes=100)\n",
        "plt.hist(returns, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNr6dtyhA9WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your code to print out the transition model at a specific coordinate with an action you want. \\\\\n",
        "1) print out the transition probabilities to all the next states given a current state $[3,3]$ and a selected action \"Up\":"
      ],
      "metadata": {
        "id": "dzvyKTbT0rP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.transition_model(env.twod_to_serial([3,3]), 1)"
      ],
      "metadata": {
        "id": "NB1LsQ670pQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Print out the transition probabilities to all the next states given a current state [7,4] and a selected action \"Down\":"
      ],
      "metadata": {
        "id": "U3mOHNAW3_2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.transition_model(env.twod_to_serial([7,4]), 2)"
      ],
      "metadata": {
        "id": "R4Y1eipc3qec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) print out the transition probabilities to all the next states given a current state $[4,9]$ and a selected action \"Down\":"
      ],
      "metadata": {
        "id": "q8a_3Dfl4FRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.transition_model(env.twod_to_serial([4,9]), 2)"
      ],
      "metadata": {
        "id": "9VfzdYSS1O8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (b) Environmental Interactions\n",
        "\n",
        "\n",
        "Following the problem description on the assignment PDF, implement ***compute_reward***, ***is_done***, and ***step*** functions. Here, \n",
        "\n",
        "*   ***is_done*** function determines wether the given next_state terminates the episode or not. \n",
        "*   ***step*** function updates its *self.observation* to the next state.\n",
        "\n",
        "After implementation, please run the cell below to see the histogram of returns.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ii01UoZFBXLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import register\n",
        "if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "  del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.05, 'size':[8, 10], 'obstacle':[32, 43, 62, 63, 44, 35, 26, 58, 49,41,57,61,31]}\n",
        ")\n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "trajs = collect_traj(env, policy=dummy_policy, num_episodes=300)\n",
        "plot_trajs(env, trajs)\n",
        "\n",
        "returns = eval_policy(env, policy=dummy_policy, num_episodes=300)\n",
        "plt.hist(returns, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PbCpR0m66m1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Dynamic Programming (DP)"
      ],
      "metadata": {
        "id": "Q0Fj-ogmf-aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! You can now run an agent to test your MDP implementation."
      ],
      "metadata": {
        "id": "O0Pj9gjjdq3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ronn8mOIxeqr"
      },
      "outputs": [],
      "source": [
        "# You can save a video of 10 episodes that visualizes the average of the returns \n",
        "# based on the current (dummy) policy.\n",
        "saved_run, returns = save_video_of_model(\"Gridworld-v0\", model=dummy_policy, num_episodes=10)\n",
        "HTML(play_video(saved_run))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2.1) Value Iteration \n",
        "You implement and analyze the value iteration (VI) algorithm filling out ***value_iteration*** and ***get_action*** functions. Please, check the assignment PDF for more details. \n",
        "\n",
        "* Hint: you may need to implement your own ***arg_max*** function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6N2tAosd84l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-D-iWHBHT3X"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "    \"\"\"\n",
        "    Value Iteration\n",
        "    \"\"\"\n",
        "    def __init__(self, env,  theta=0.00001, discount_factor=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the VI class\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        env: object\n",
        "            an OpenAI Gym compatible environment\n",
        "        theta: float\n",
        "            a max error (termination) threshold \n",
        "        discount_factor: float\n",
        "            discount factor (i.e., gamma)\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        probs: numpy array with a length of {size of state space}\n",
        "            probabilities of transition to the next_state ...                    \n",
        "        \"\"\"      \n",
        "        self.env = env\n",
        "        self.V = np.zeros(env.observation_space.n)\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta= theta   # Maximum error thresold value\n",
        "    \n",
        "    def value_iteration(self):\n",
        "        \"\"\"\n",
        "        A value iteration function. Until the error bound reaches the threshold \n",
        "        (theta), The value table is updated by the Dynamic Programming \n",
        "        (refer to the lecture)\n",
        "        \"\"\"\n",
        "        errors = []\n",
        "        episode_rewards = []\n",
        "        while True:\n",
        "            max_error = 0\n",
        "            for state in range(env.observation_space.n):\n",
        "\n",
        "            ###################################################################\n",
        "            #####################   PLACE YOUR CODE HERE   ####################\n",
        "            ###                                                             ###\n",
        "            ### Instruction                                                 ###\n",
        "            ### -----------                                                 ###\n",
        "            ### Fill out self.V by using the VI algorithm                   ###      \n",
        "            ###\n",
        "            ### Example                                                     ###\n",
        "            ### -----------                                                 ###\n",
        "            ### Initializations\n",
        "            ### For a in available actions\n",
        "            ###   For s' in available next states\n",
        "            ###     Compute reward(s,a,s'), etc\n",
        "            ###     Compute action value \n",
        "            ### Fill out \"self.V\" at the current state using the action values\n",
        "\n",
        "            ###                                                             ###\n",
        "            ###################################################################\n",
        "            ###################################################################\n",
        "            errors.append(max_error)\n",
        "            mean_ep_reward = np.mean( eval_policy(env, policy=self.get_action, num_episodes=10))\n",
        "            episode_rewards.append(mean_ep_reward)\n",
        "\n",
        "            if max_error < self.theta:\n",
        "                break\n",
        "        return episode_rewards, errors\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Return the best action. \n",
        "        HINT: how do you handle if there are multiple actions with the highest value?\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        action: integer\n",
        "            an action index\n",
        "        \"\"\"\n",
        "\n",
        "        action = None\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### Find the best \"action\" that maximize Q value                ###      \n",
        "        ###                                                             ###  \n",
        "        ### Example                                                     ###\n",
        "        ### -----------                                                 ###\n",
        "        ### Compute Q values when you apply each action                 ###\n",
        "        ### For a in available actions\n",
        "        ###   For s' in next states\n",
        "        ###     Q[a] = ?  \n",
        "        ###\n",
        "        ### action = ?\n",
        "        \n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementation, you can check your implementation with the following code."
      ],
      "metadata": {
        "id": "9-amthiWoG9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF8Y1jTj6sD2"
      },
      "outputs": [],
      "source": [
        "##### Codes for the running the policy (Do not Modify) #####\n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "env.epsilon = 0.05\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors = vi.value_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check the state values of the first 10 states:"
      ],
      "metadata": {
        "id": "XPrDvLDG8nPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the values of the first 10 states of the gridworld environment\n",
        "print(vi.V[:10])"
      ],
      "metadata": {
        "id": "qAHA2FXV8zon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also overlay the best action at each state and plot the distribution of trajectories produced by the trained VI policy:"
      ],
      "metadata": {
        "id": "B9FLE-H9851Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Visualize the policy with the video and trajectories plot\n",
        "saved_run, _ = save_video_of_model(\"Gridworld-v0\", vi.get_action)\n",
        "render_value_map_with_action(env, vi.V, vi.get_action)\n",
        "HTML(play_video(saved_run))\n",
        "trajs = collect_traj(env, policy=vi.get_action, num_episodes=100)\n",
        "plot_trajs(env, trajs)"
      ],
      "metadata": {
        "id": "uYaBQzB09Dm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also plot the expected returns and errors with respect to the number of iterations until convergence:"
      ],
      "metadata": {
        "id": "VvmtJXgi9luH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot error and mean episodic rewards per iterations\n",
        "ax = plt.subplot(2,1,1)\n",
        "ax.plot(ep_rews)\n",
        "bx = plt.subplot(2,1,2)\n",
        "bx.plot(errors)\n",
        "ax.set_ylabel('Mean episode rewards')\n",
        "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
        "bx.set_xlabel('Number of iterations')\n",
        "bx.set_ylabel('Errors')\n",
        "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
        "empty_string_labels = ['']*len(labels)\n",
        "ax.set_xticklabels(empty_string_labels)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9nSnT23o9Lk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2.2) Comparison under different transition models\n",
        "\n",
        "Here, you need to vary $\\epsilon$. Get the necessary results when $\\epsilon=0.05$:"
      ],
      "metadata": {
        "id": "g9c9LZNnAnwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import register\n",
        "if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.05, 'size':[8, 10], 'obstacle':[32, 43, 62, 63, 44, 35, 26, 58, 49,41,57,61,31]}\n",
        ")                                                       \n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors = vi.value_iteration()\n",
        "###################################################################\n",
        "\n",
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "### 1. plot the expected return                                 ###\n",
        "### 2. Overlay the best action                                  ###\n",
        "### 3. Plot the distribution of trajectories                    ###\n",
        "\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "GQMIJi_SAf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the necessary results when $\\epsilon=0.2$:"
      ],
      "metadata": {
        "id": "MxpQTJ3l_ZLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import register\n",
        "if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.2, 'size':[8, 10], 'obstacle':[32, 43, 62, 63, 44, 35, 26, 58, 49,41,57,61,31]}\n",
        ")                                                \n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors = vi.value_iteration()\n",
        "###################################################################\n",
        "\n",
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "### 1. plot the expected return                                 ###\n",
        "### 2. Overlay the best action                                  ###\n",
        "### 3. Plot the distribution of trajectories                    ###\n",
        "\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "otQ4swav_YjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}