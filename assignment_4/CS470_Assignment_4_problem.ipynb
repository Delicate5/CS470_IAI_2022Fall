{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pidipidi/CS470_IAI_2022Fall/blob/main/assignment_4/CS470_Assignment_4_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS470 Assignment 4\n",
        "In this assignment, we will implement on/off-policy-based temporal difference (TD) methods: SARSA and Q-learning. Then, as an advanced step, we adopt a deep Q-learning algorithm to OpenAI Gym environments. Note that you must run this file on Google Chrome. "
      ],
      "metadata": {
        "id": "UtOBF0bYZWKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements for initialization\n",
        "We will first install dependencies and declare auxiliary functions for visualization"
      ],
      "metadata": {
        "id": "tHYpkF-KbM1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMLNlciUp9y1"
      },
      "outputs": [],
      "source": [
        "#Install some dependencies for visualizing the agents\n",
        "!pip install pyglet==1.5.1  &> /dev/null\n",
        "!apt install -y python-opengl ffmpeg xvfb &> /dev/null\n",
        "!pip install pyvirtualdisplay &> /dev/null\n",
        "!pip install gym==0.24.0 &> /dev/null \n",
        "!pip install numpy &> /dev/null\n",
        "\n",
        "!pip install pickle5 &> /dev/null\n",
        "!pip install pyyaml==6.0 &> /dev/null \n",
        "!pip install imageio imageio_ffmpeg &> /dev/null\n",
        "\n",
        "!apt-get install -y python x11-utils &> /dev/null\n",
        "!pip install scikit-video ffio pyrender &> /dev/null\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "import imageio, random, copy\n",
        "import sys, time, os, base64, io\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import IPython, functools, matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as Image\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def eval_model(env_name, model=None, max_episodes=10):\n",
        "    \"\"\"\n",
        "    Compute the average of the sum of rewards\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    returns = 0\n",
        "    while num_runs < max_episodes:\n",
        "        if \"Gridworld\" in env_name:\n",
        "            input_obs = obs\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown env for saving: {env_name}\")\n",
        "\n",
        "        if model is not None:\n",
        "            action = model(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        prev_obs = obs\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        returns += reward\n",
        "        if done:\n",
        "            num_runs += 1\n",
        "            obs = env.reset()\n",
        "\n",
        "    return returns / num_runs\n",
        "\n",
        "def render_value_map_with_action(env, Q, policy=None):\n",
        "    '''\n",
        "    Render a state (or action) value grid map.\n",
        "    V[s] = max(Q[s,a])\n",
        "    '''\n",
        "    Q = Q.copy()\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    if len(np.shape(Q))>1:\n",
        "        V = np.amax(Q, axis=1) \n",
        "        V = V.reshape((n,m))\n",
        "    else:\n",
        "        V = Q.reshape((n,m))\n",
        "    import itertools\n",
        "    symbol = ['.', '^','v', '<', '>']\n",
        "    x = range(0, env.grid_map_shape[0]+1)\n",
        "    y = range(0, env.grid_map_shape[1]+1)\n",
        "\n",
        "    min_val = V[0,0]\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle // env.grid_map_shape[1]\n",
        "        posy = obstacle % env.grid_map_shape[1]\n",
        "        V[posx, posy] = min_val\n",
        "        obstacles[posx, posy] = 1\n",
        "\n",
        "    plt.imshow(V, cmap='jet', interpolation='nearest')\n",
        "    for s in range(env.observation_space.n):\n",
        "        twod_state = env.serial_to_twod(s)\n",
        "        state_inds = s\n",
        "        best_action = policy(s)\n",
        "        plt.plot([twod_state[1]], [twod_state[0]], marker=symbol[best_action], linestyle='none', color='k')\n",
        "\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "            \n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def collect_traj(env, policy=None, num_episodes=10):\n",
        "    \"\"\"Collect trajectories (rollouts) following the input policy\"\"\"\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    episode_rewards = []\n",
        "    episode_reward = 0\n",
        "    traj = []\n",
        "    trajs = []\n",
        "\n",
        "    while num_runs < num_episodes:\n",
        "        input_obs = obs\n",
        "        if policy is not None:\n",
        "            action = policy(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        traj.append(obs)\n",
        "        prev_obs = obs\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            num_runs += 1\n",
        "            traj.append(obs)\n",
        "            trajs.append(traj)\n",
        "            traj = []\n",
        "            obs = env.reset()\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "    return trajs#, episode_rewards\n",
        "\n",
        "def plot_trajs(env, trajectories):\n",
        "    \"\"\"Plot the input trajectories\"\"\"\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    V = np.zeros([n,m])\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle // env.grid_map_shape[1]\n",
        "        posy = obstacle % env.grid_map_shape[1]\n",
        "        obstacles[posx, posy] = 1\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    for trajectory in trajectories:\n",
        "        traj_2d = np.array([ env.serial_to_twod(s) for s in trajectory ])\n",
        "        y = traj_2d[:, 0]\n",
        "        x = traj_2d[:, 1]\n",
        "        plt.plot(x, y, alpha=0.1, color='r')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TdTSLSV1U9DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import register\n",
        "\n",
        "class BaseGridEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "    def __init__(self, size=[8,10], start=None ,goal=None, epsilon=0.0, obstacle=None):\n",
        "        \"\"\"\n",
        "        An initialization function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: a list of integers\n",
        "            the dimension of 2D grid environment\n",
        "        start: integer\n",
        "            start state (i.e., location)\n",
        "        epsilon: float\n",
        "            the probability of taking random actions\n",
        "        obstacle: \n",
        "\n",
        "        \"\"\"\n",
        "        self.grid_map_shape = [size[0], size[1]]  # The size of the map\n",
        "        self.epsilon = epsilon  # action-failure probability\n",
        "        self.obstacles = obstacle   # list of states stating position of the obstacles\n",
        "        \n",
        "        ''' set observation space and action space '''\n",
        "        self.observation_space = spaces.Discrete( size[0] * size[1])\n",
        "        self.action_space = spaces.Discrete( 5 )\n",
        "        self.start_state = start if start is not None else 0 \n",
        "        if goal is None:\n",
        "            self.terminal_state = size[0] * size[1] - 1\n",
        "        else:\n",
        "            self.terminal_state = goal\n",
        "\n",
        "    def serial_to_twod(self, ind):\n",
        "        \"\"\"Convert a serialized state number to a 2D map's state coordinate\"\"\"\n",
        "        return np.array( [ ind // self.grid_map_shape[1], ind % self.grid_map_shape[1]])\n",
        "\n",
        "    def twod_to_serial(self, twod):\n",
        "        \"\"\"Convert a 2D map's state coordinate to a serialized state number\"\"\"\n",
        "        return np.array( twod[0]* self.grid_map_shape[1] + twod[1])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Rest the environment by initializaing the start state \"\"\"\n",
        "        self.observation = self.start_state\n",
        "        return self.observation\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        \"\"\"Render the agent state\"\"\"\n",
        "        pixel_size = 20\n",
        "        img = np.zeros([ pixel_size * self.grid_map_shape[0], pixel_size * self.grid_map_shape[1],3])\n",
        "        for obstacle in self.obstacles:\n",
        "          pos_x, pos_y = self.serial_to_twod(obstacle)\n",
        "          img[pixel_size*pos_x: pixel_size*(1+pos_x), pixel_size*pos_y: pixel_size*(1+pos_y)] += [255,0,0]\n",
        "        agent_state = self.serial_to_twod(self.observation)   \n",
        "        agent_target_state = self.serial_to_twod(self.terminal_state)\n",
        "        img[pixel_size*agent_state[0]: pixel_size*(1+agent_state[0]), pixel_size*agent_state[1]: pixel_size*(1+agent_state[1])] += [0,0,255]\n",
        "        img[pixel_size*agent_target_state[0]: pixel_size*(1+agent_target_state[0]), pixel_size*agent_target_state[1]: pixel_size*(1+agent_target_state[1])] += [0,255,0]\n",
        "        if mode == 'human':\n",
        "          fig = plt.figure(0)\n",
        "          plt.clf()\n",
        "          plt.imshow(img, cmap='gray')\n",
        "          fig.canvas.draw()\n",
        "          plt.pause(0.01)\n",
        "        if mode == 'rgb_array':\n",
        "          return img\n",
        "        return \n",
        "\n",
        "    def _close_env(self):\n",
        "        \"\"\"Close the environment screen\"\"\"\n",
        "        plt.close(1)\n",
        "        return\n",
        "\n",
        "class GridEnv(BaseGridEnv):\n",
        "    \"\"\"\n",
        "    A grid-world environment.\n",
        "    \"\"\"\n",
        "    def transition_model(self, state, action):\n",
        "        \"\"\"\n",
        "        A transition model that return a list of probabilities of transitions\n",
        "        to next states when the agent select 'action' at the 'state': T(s' | s,a)\n",
        "\n",
        "        In our envrionemnt, if the state is in obstacles or in a goal, \n",
        "        it will stay in its state at any action\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index        \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        probs: numpy array with a length of {size of state space}\n",
        "            probabilities of transition to the next_state ...                    \n",
        "        \"\"\"\n",
        "        if not isinstance(state, int):\n",
        "            state = state.item()\n",
        "\n",
        "        # the transition probabilities to the next states\n",
        "        probs = np.zeros(self.observation_space.n)\n",
        "\n",
        "        # Left top is [0,0], \n",
        "        action_pos_dict = {0: [0,0], 1:[-1, 0], 2:[1,0], 3:[0,-1], 4:[0,1]}\n",
        "        \n",
        "        if state in self.obstacles or state == self.terminal_state:\n",
        "            probs[state] = 1.0\n",
        "            return probs\n",
        "        action_probs = np.ones(self.action_space.n) * self.epsilon / 4\n",
        "        action_probs[action] = 1 - self.epsilon\n",
        "               \n",
        "        for new_action, prob in enumerate(action_probs):\n",
        "            a_state = self.serial_to_twod(state)\n",
        "            done = False\n",
        "\n",
        "            nxt_agent_state = np.array([a_state[0] + action_pos_dict[new_action][0],\n",
        "                                a_state[1] + action_pos_dict[new_action][1]])\n",
        "            if new_action == 0: \n",
        "                nxt_agent_state = a_state\n",
        "            if nxt_agent_state[0] < 0 or nxt_agent_state[0] >= self.grid_map_shape[0]:\n",
        "                nxt_agent_state = a_state\n",
        "            if nxt_agent_state[1] < 0 or nxt_agent_state[1] >= self.grid_map_shape[1]:\n",
        "                nxt_agent_state = a_state\n",
        "            next_state = self.twod_to_serial(nxt_agent_state)\n",
        "\n",
        "            probs[next_state] += prob\n",
        "\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def compute_reward(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        A reward function that returns the total reward after selecting 'action'\n",
        "        at the 'state'. In this environment, \n",
        "        (a) If it reaches a goal state, it terminates returning a reward of +10\n",
        "        (b) If it reaches an obstacle, it terminates returning a penalty of -5\n",
        "        (c) For any action, it add a step penalty of -0.1\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        reward: float\n",
        "            a total reward value\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0\n",
        "        reward = 10.0 if next_state == self.terminal_state else 0\n",
        "        if next_state in self.obstacles:\n",
        "            reward = -5\n",
        "        reward -= 0.1\n",
        "\n",
        "        return reward\n",
        "    \n",
        "    def is_done(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        Return True when the agent is in a terminal state or obstacles, \n",
        "        otherwise return False\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        done: Bool\n",
        "            the result of termination or collision\n",
        "        \"\"\"\n",
        "        done = next_state in self.obstacles or next_state == self.terminal_state\n",
        "        return done \n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        A step function that applies the input action to the environment.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        action: integer\n",
        "            action index         \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        observation: integer\n",
        "            the outcome of the given action (i.e., next state)... s' ~ T(s'|s,a)\n",
        "        reward: float\n",
        "            the reward that would get for ... r(s, a, s')\n",
        "        done: Bool\n",
        "            the result signal of termination or collision\n",
        "        info: Dictionary\n",
        "            Information dictionary containing miscellaneous information...\n",
        "            (Do not need to implement info)\n",
        "\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        action = int(action)\n",
        "        \n",
        "        probs = self.transition_model(self.observation, action)\n",
        " \n",
        "        next_state = np.random.choice(self.observation_space.n, 1, p=probs).item()\n",
        "        self.agent_state = self.serial_to_twod(next_state)\n",
        "        old_obs = self.observation\n",
        "        self.observation = next_state\n",
        "        reward = self.compute_reward(old_obs, action, self.observation)\n",
        "        done = self.is_done(old_obs, action, self.observation)\n",
        "        \n",
        "        return (self.observation, reward, done, {})\n"
      ],
      "metadata": {
        "id": "dUVZhsTDeBxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1. On-policy Algorithm: SARSA\n",
        "In this problem, you implement a SARSA algorithm, filling in *learn()* and *get_action()* functions. For details, please see the assignment PDF."
      ],
      "metadata": {
        "id": "eF0HqrPIg4RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, env, epsilon=0.1, learning_rate=0.05):\n",
        "        \"\"\"\n",
        "        Initialize the SARSA class\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        env: object\n",
        "            an OpenAI Gym compatible environment\n",
        "        epsilon: float\n",
        "            a probability of running random policy \n",
        "        learning_rate: float\n",
        "            the update step size (i.e., alpha)    \n",
        "        \"\"\"\n",
        "        self.env     = env\n",
        "        self.actions = range(self.env.action_space.n)\n",
        "        self.learning_rate   = learning_rate\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
        "\n",
        "    def learn(self, state, action, reward, done, next_state, next_action):\n",
        "        \"\"\"\n",
        "        Update the current q_table given (s,a,r,s',a') pairs.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "        action: integer\n",
        "        reward: float\n",
        "        done: Boolean\n",
        "        next_state: integer\n",
        "        next_action: integer\n",
        "        \"\"\"\n",
        "        if type(state) is not int:\n",
        "            state = state.item()\n",
        "        if type(next_state) is not int:\n",
        "            next_state = next_state.item()\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### implement the Q-function update part of an one-step TD(0)   ###\n",
        "        ### prediction                                                  ###\n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "                \n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"\n",
        "        Return an action following the epsilon-greedy policy.\n",
        "        Note that this function returns the greedy action when \n",
        "        deterministic flag is True.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state:         Integer\n",
        "        deterministic: Boolean\n",
        "            disable the epsilon-gree policy if True\n",
        "\n",
        "        Returns\n",
        "        ----------        \n",
        "        action: Integer\n",
        "            the action index\n",
        "        \"\"\"\n",
        "        if type(state) is not int:\n",
        "            state = state.item()\n",
        "        if deterministic:\n",
        "            epsilon = 0\n",
        "        else: \n",
        "            epsilon = self.epsilon\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### select an action following the epsilon-greedy policy        ###\n",
        "        ### Hint: you need to *randomly* select an action given the same###\n",
        "        ###       value of possible actions                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "1Nq0C0ntK_4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you need to implement a training loop for the SARSA algorithm, which samples the transition pair $(s, a, r, s', a')$ and updates the Q function.\n"
      ],
      "metadata": {
        "id": "H0u6zjh5hA86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid world environment\n",
        "if 'Gridworld-v1' in gym.envs.registration.registry.env_specs:\n",
        "  del gym.envs.registration.registry.env_specs['Gridworld-v1']\n",
        "register(\n",
        "    id='Gridworld-v1',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=40,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.0, 'size':[6, 8], 'start': 40, 'goal': 47,\n",
        "            'obstacle':[42, 43, 44, 45, 26, 27, 28, 29]}    \n",
        ")\n",
        "\n",
        "# Train the SARSA algorithm\n",
        "env = gym.make('Gridworld-v1')\n",
        "sarsa_agent = SARSA(env, epsilon=0.3, learning_rate=0.01)\n",
        "sarsa_reward_list = []\n",
        "for episode in range(10000):\n",
        "    state = env.reset()\n",
        "\n",
        "    ###################################################################\n",
        "    #####################   PLACE YOUR CODE HERE   ####################\n",
        "    ###                                                             ###\n",
        "    ### Instruction                                                 ###\n",
        "    ### -----------                                                 ###\n",
        "    ### Estimate the action-value form of the TD error              ###\n",
        "    ###                                                             ###\n",
        "    ### Example                                                     ###\n",
        "    ### -------                                                     ###\n",
        "    ### action = ?                                                  ###    \n",
        "    ### while True:                                                 ###        \n",
        "    ###     next_state ? = env.step()                               ###            \n",
        "    ###     next_action ?                                           ###            \n",
        "    ###     agent.learn()                                           ###            \n",
        "    ###     .....                                                   ###                \n",
        "    ###     (when do we break?)                                     ###               \n",
        "    ###                                                             ###\n",
        "    ###################################################################\n",
        "    ###################################################################\n",
        "\n",
        "    if episode %100 == 0:\n",
        "        rewards = eval_model(\"Gridworld-v1\", sarsa_agent.get_action, max_episodes=100)\n",
        "        sarsa_reward_list.append(rewards)\n",
        "    if episode %2000 == 0:\n",
        "        if episode > 0:\n",
        "            plt.close()\n",
        "        # Value map with action visualization\n",
        "        render_value_map_with_action(env, sarsa_agent.q_table, policy=lambda s: sarsa_agent.get_action(s,True))\n",
        "\n",
        "# Sum of rewards during episode        \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(sarsa_reward_list, color='blue')\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Sum of rewards during episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_aaESXxK_x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2. Off-policy Algorithm: Q-Learning\n",
        "In this problem, you implement a Q-Learning algorithm, filling in *learn()* and *get_action()* functions. For details, please see the assignment PDF."
      ],
      "metadata": {
        "id": "L9bxY_BBhD1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class QLearning:\n",
        "    \"\"\"Q-Learning Algorithm \"\"\"\n",
        "    def __init__(self, env, epsilon=0.1, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the Q-Learning class\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        env: object\n",
        "            an OpenAI Gym compatible environment\n",
        "        epsilon: float\n",
        "            a probability of running random policy \n",
        "        learning_rate: float\n",
        "            the update step size (i.e., alpha)    \n",
        "        \"\"\"      \n",
        "        self.env = env\n",
        "        self.actions = range(self.env.action_space.n)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Update the current q_table according to the max bellman update rules\n",
        "        given (s,a,r,s') pairs.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "        action: integer\n",
        "        reward: float\n",
        "        done: Boolean\n",
        "        next_state: integer\n",
        "        \"\"\"      \n",
        "        if type(state) is not int:\n",
        "            state = state.item()\n",
        "        if type(next_state) is not int:\n",
        "            next_state = next_state.item()\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### implement the Q-function update part of an one-step TD(0)   ###\n",
        "        ### prediction                                                  ###\n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"\n",
        "        Return an action following the epsilon-greedy policy.\n",
        "        Note that this function returns the greedy action when \n",
        "        deterministic flag is True.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state:         Integer\n",
        "        deterministic: Boolean\n",
        "            disable the epsilon-gree policy if True\n",
        "\n",
        "        Returns\n",
        "        ----------        \n",
        "        action: Integer\n",
        "            the action index\n",
        "        \"\"\"      \n",
        "        if type(state) is not int:\n",
        "            state = state.item()\n",
        "        if deterministic:\n",
        "            epsilon = 0\n",
        "        else: \n",
        "            epsilon = self.epsilon\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### select an action following the epsilon-greedy policy        ###\n",
        "        ### Hint: you need to *randomly* select an action given the same###\n",
        "        ###       value of possible actions                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "jUBWrzqrZatB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you need to implement a training loop for the Q-Learning algorithm, which samples the transition pair $(s, a, r, s')$ and updates the Q function."
      ],
      "metadata": {
        "id": "7AaB0Fgm4isz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Q-Learning algorithm\n",
        "ql_agent = QLearning(env, epsilon=0.3)\n",
        "ql_reward_list = []\n",
        "for episode in range(10000):\n",
        "    state = env.reset()\n",
        "\n",
        "    ###################################################################\n",
        "    #####################   PLACE YOUR CODE HERE   ####################\n",
        "    ###                                                             ###\n",
        "    ### Instruction                                                 ###\n",
        "    ### -----------                                                 ###\n",
        "    ### Estimate the action-value form of the TD error              ###\n",
        "    ###                                                             ###\n",
        "    ### Example                                                     ###\n",
        "    ### -------                                                     ###\n",
        "    ### while True:                                                 ###        \n",
        "    ###     action ?                                                ###            \n",
        "    ###     next_state, ? = env.step()                              ###            \n",
        "    ###     agent.learn()                                           ###            \n",
        "    ###     .....                                                   ###         \n",
        "    ###                                                             ###\n",
        "    ###################################################################\n",
        "    ###################################################################            \n",
        "    \n",
        "    if episode %100 == 0:\n",
        "        rewards = eval_model(\"Gridworld-v1\", ql_agent.get_action, max_episodes=100)\n",
        "        ql_reward_list.append(rewards)\n",
        "    if episode %2500 == 0:\n",
        "        if episode > 0:\n",
        "            plt.close()\n",
        "        render_value_map_with_action(env, ql_agent.q_table, policy=lambda s: ql_agent.get_action(s,True))\n",
        "        \n",
        "# Sum of rewards during episode        \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(ql_reward_list, color='orange')\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Sum of rewards during episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mwheg0oaZz57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3. Comparison between SARSA and Q-Learning algorithms\n",
        "\n",
        "In this section, you are asked to compare the SARSA and the Q-learning algorithm on the environment used in previous subproblems. If necessary, you can select $\\epsilon$ values and obstacle configurations to see how SARSA and Q-learning algorithms differ from each other. Please, attach a plot of the sum of rewards over episode for the two algorithms."
      ],
      "metadata": {
        "id": "SgyYqUZU5DnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot trajectories generated by the SARSA agent\n",
        "trajs = collect_traj(env, policy=lambda s: sarsa_agent.get_action(s,True), num_episodes=300)\n",
        "plot_trajs(env, trajs)"
      ],
      "metadata": {
        "id": "WCdRlaWJVJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot trajectories generated by the Q-learning agent\n",
        "trajs = collect_traj(env, policy=lambda s: ql_agent.get_action(s,True), num_episodes=300)\n",
        "plot_trajs(env, trajs)"
      ],
      "metadata": {
        "id": "RNOYUYUHXav3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(sarsa_reward_list, color='blue')\n",
        "plt.plot(ql_reward_list, color='orange')\n",
        "plt.ylim(-6, 10)\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Sum of rewards during episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkP_Vgr8KxrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Deep Q-learning with Applications"
      ],
      "metadata": {
        "id": "LMuk1H46Nw_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "We will first install dependencies and declare auxiliary functions for visualization."
      ],
      "metadata": {
        "id": "XeHxHTFSSfz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install some dependencies for box2d simulation\n",
        "!pip install stable-baselines3[extra] box2d box2d-kengz &> /dev/null\n",
        "!pip install gym==0.24.0 gym[box2d] &> /dev/null\n",
        "\n",
        "import skvideo.io\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "def save_video_of_model(env_name, policy=None, suffix=\"\"):\n",
        "    \"\"\"Record an agent behavior in an input environment\"\"\"\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs = env.reset()\n",
        "    prev_obs = obs\n",
        "\n",
        "    filename = env_name + suffix + \".mp4\"\n",
        "    output_video = skvideo.io.FFmpegWriter(filename)\n",
        "\n",
        "    counter = 0\n",
        "    frame_rate = 2\n",
        "    done = False\n",
        "    if hasattr(env, \"render_mode\"):\n",
        "        env.render_mode=\"rgb_array\"\n",
        "    while not done and counter < 200:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        if counter % frame_rate == 0:\n",
        "            output_video.writeFrame(frame)\n",
        "\n",
        "        input_obs = obs\n",
        "\n",
        "        if policy is not None:\n",
        "            action = policy(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        if \"FrozenLake\" in env_name:\n",
        "            action = action.item()\n",
        "        prev_obs = obs\n",
        "        \n",
        "        obs, reward, done, info = env.step(action)\n",
        "        counter += 1\n",
        "\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    output_video.writeFrame(frame)\n",
        "    output_video.close()\n",
        "    print(\"Successfully saved {} frames into {}!\".format(counter, filename))\n",
        "    return filename\n",
        "\n",
        "def play_video(filename, width=None):\n",
        "    \"\"\"Play the filename of video\"\"\"\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "    \n",
        "    mp4 = open(filename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    source = \"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url\n",
        "    return source"
      ],
      "metadata": {
        "id": "uZARKQGyNm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Practice: Deep Q-Network (DQN) with *gridworld* environment\n",
        "Okay, you will now adopt a pre-implemented DQN in the ***stable baseline3*** library on the gridworld environment. Let's see if we can train a DQN-based agent!"
      ],
      "metadata": {
        "id": "lHW9MwBmSiy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps, EvalCallback"
      ],
      "metadata": {
        "id": "2tt-sIW4ORYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "##  You can choose your Algorithm / Environment / seed as you want  ##\n",
        "######################################################################\n",
        "\n",
        "policy_cls = DQN\n",
        "env_id = \"Gridworld-v1\"\n",
        "SEED = 47\n",
        "\n",
        "######################################################################\n",
        "# Use a separate environment for evaluation\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "model = policy_cls(policy='MlpPolicy',  \n",
        "            env=env, \n",
        "            seed=SEED, \n",
        "\n",
        "            learning_starts=0,              # Decide warming up steps\n",
        "            batch_size=32,                  # Batch size for the neural network\n",
        "            learning_rate=3e-4,             # Learning rate for the neural network\n",
        "            buffer_size=30000,              # The size of stored transitions from the past\n",
        "\n",
        "            exploration_initial_eps=1.0,    # Exploration rate will be \n",
        "            exploration_fraction=0.2,       # gradually descreasing from exploration_inital_eps to exploration_final_eps \n",
        "            exploration_final_eps= 0.1,     # for exploration_fraction amount of times\n",
        "                   \n",
        "            target_update_interval= 250,    # Update interval of the target neural network\n",
        "            train_freq=4,                   # How often the neural netwok to be updated (once per N steps)\n",
        "            gradient_steps= -1,             # Number of updates per batch  (if -1, set to batch_size)\n",
        "            policy_kwargs= dict(net_arch=[64, 64]),\n",
        "                                            # The shape of the neural network \n",
        "            gamma=0.9                       # Discount Factor\n",
        "          )\n",
        "\n",
        "reward_list = []\n",
        "class CB(EvalCallback):\n",
        "    \"\"\"Callback for evaluation\"\"\"\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5)\n",
        "            reward_list.append(mean_reward)\n",
        "            print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "        return True\n",
        "\n",
        "cb = CB(eval_env=eval_env, eval_freq=1000)\n",
        "model.learn(total_timesteps=20000, callback=cb)\n",
        "\n",
        "# Visualization\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(reward_list, color='blue')\n",
        "ax.set_xlabel('Number of timesteps ($\\\\times 10^3$)')\n",
        "ax.set_ylabel('Sum of rewards during episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kXVPTLrSN1Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Plot Value map and best action #######\n",
        "import torch as th\n",
        "total_states = th.arange(env.observation_space.n)\n",
        "Q = model.q_net(total_states).cpu().detach().numpy()\n",
        "Q = Q.copy()\n",
        "render_value_map_with_action(env, Q, policy=lambda *x: model.predict(*x, deterministic=True)[0])"
      ],
      "metadata": {
        "id": "JL_qi4qB-m-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_video_of_model(env_id, policy=lambda *x: model.predict(*x, deterministic=True)[0])\n",
        "\n",
        "from IPython.display import HTML\n",
        "source = play_video(filename=f'{env_id}.mp4')\n",
        "HTML(source)"
      ],
      "metadata": {
        "id": "T01xfOwtL86W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Practice: DQN with other gym environment\n",
        "Now, you need to run DQN with three other gym environment.\n",
        "Can you train DQN per environment and show the training results with the reward curve? \n",
        "For gym environment, please look at following *env_id*: \n",
        "*   [Acrobot-v1](https://www.gymlibrary.dev/environments/classic_control/acrobot/)\n",
        "*   [LunarLander-v2](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
        "*   [MountainCar-v0](https://www.gymlibrary.dev/environments/classic_control/mountain_car/)\n",
        "\n",
        "If you need further clarification, you can visit the link above. You may look for hyperparameters for better results: [Stable_baselines3_zoo](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml). \n",
        "\n",
        "You need to achieve certain level of rewards to be regarded as successful training.\n",
        "*   Acrobot-v1: Above -100\n",
        "*   LunarLander-v2: Above 100\n",
        "*   MountainCar-v0: Above -150\n",
        "\n",
        "You may increase the total_timesteps up to 200000."
      ],
      "metadata": {
        "id": "lQhC2Pt4YrSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use (Acrobot-v1, LunarLander-v2, MountainCar-v0) environment\n",
        "policy_cls = DQN\n",
        "\n",
        "######################################################################\n",
        "#####################   PLACE YOUR CODE HERE   #######################\n",
        "### Instruction                                                    ###\n",
        "### -----------                                                    ###\n",
        "### You define environment and model that you want.                ###\n",
        "###                                                                ###\n",
        "### Example                                                        ###\n",
        "### -----------                                                    ###\n",
        "### env_id = ?                                                     ###\n",
        "### env = ?                                                        ###\n",
        "### ...                                                            ###\n",
        "### model = policy_cls(?)                                          ###\n",
        "###                                                                ###\n",
        "######################################################################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "reward_list = []\n",
        "class CB(EvalCallback):\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=10)\n",
        "            reward_list.append(mean_reward)\n",
        "            print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "        return True\n",
        "\n",
        "cb = CB(eval_env=eval_env, eval_freq=10000)\n",
        "model.learn(total_timesteps=150000, callback=cb)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(reward_list, color='blue')\n",
        "ax.set_xlabel('Episodes ($\\\\times 10^4$)')\n",
        "ax.set_ylabel('Sum of rewards during episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ph3EOFFLOcEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can record a video!"
      ],
      "metadata": {
        "id": "EVEEH3t75K6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_video_of_model(env_id, policy=lambda *x: model.predict(*x, deterministic=True)[0])\n",
        "\n",
        "from IPython.display import HTML\n",
        "source = play_video(filename=f'{env_id}.mp4')\n",
        "HTML(source)"
      ],
      "metadata": {
        "id": "TyxRtoHSYSzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}